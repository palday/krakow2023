
@article{lakens_calculating_2013,
	title = {Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and {ANOVAs}},
	volume = {4},
	issn = {1664-1078},
	shorttitle = {Calculating and reporting effect sizes to facilitate cumulative science},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00863/abstract},
	doi = {10.3389/fpsyg.2013.00863},
	urldate = {2020-10-27},
	journal = {Frontiers in Psychology},
	author = {Lakens, Daniël},
	year = {2013},
}

@article{bates_complexity_2019,
	title = {Complexity in fitting {Linear} {Mixed} {Models}},
	url = {https://nextjournal.com/doi/10.33016/nextjournal.100002},
	doi = {10.33016/nextjournal.100002},
	abstract = {Linear mixed-effects models are increasingly used for the analysis of data from experiments in fields like psychology where several subjects are each exposed to each of several different items. In addition to a response, which here will be assumed to be on a continuous scale, such as a response time, a number of experimental conditions are systematically varied during the experiment. In the language of statistical experimental design the latter variables are called experimental factors whereas factors like Subject and Item are blocking factors. That is, these are known sources of variation that usually are not of interest by themselves but still should be accounted for when looking for systematic variation in the response.},
	language = {en},
	urldate = {2020-10-27},
	journal = {Nextjournal},
	author = {Bates, Douglas M., Bates},
	month = aug,
	year = {2019},
}

@article{bates_parsimonious_2018,
	title = {Parsimonious {Mixed} {Models}},
	url = {http://arxiv.org/abs/1506.04967},
	abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. Recently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification.},
	urldate = {2020-10-27},
	journal = {arXiv:1506.04967 [stat]},
	author = {Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
	month = may,
	year = {2018},
	note = {arXiv: 1506.04967},
	keywords = {Statistics - Methodology},
}

@article{baayen_cave_2017,
	title = {The cave of shadows: {Addressing} the human factor with generalized additive mixed models},
	volume = {94},
	issn = {0749-596X},
	shorttitle = {The cave of shadows},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X16302467},
	doi = {10.1016/j.jml.2016.11.006},
	abstract = {Generalized additive mixed models are introduced as an extension of the generalized linear mixed model which makes it possible to deal with temporal autocorrelational structure in experimental data. This autocorrelational structure is likely to be a consequence of learning, fatigue, or the ebb and flow of attention within an experiment (the ‘human factor’). Unlike molecules or plots of barley, subjects in psycholinguistic experiments are intelligent beings that depend for their survival on constant adaptation to their environment, including the environment of an experiment. Three data sets illustrate that the human factor may interact with predictors of interest, both factorial and metric. We also show that, especially within the framework of the generalized additive model, in the nonlinear world, fitting maximally complex models that take every possible contingency into account is ill-advised as a modeling strategy. Alternative modeling strategies are discussed for both confirmatory and exploratory data analysis.},
	language = {en},
	urldate = {2020-10-27},
	journal = {Journal of Memory and Language},
	author = {Baayen, Harald and Vasishth, Shravan and Kliegl, Reinhold and Bates, Douglas},
	month = jun,
	year = {2017},
	keywords = {Autocorrelation, Confirmatory versus exploratory data analysis, Experimental time series, Generalized additive mixed models, Model selection, Within-experiment adaptation},
	pages = {206--234},
}

@article{bates_fitting_2015,
	title = {Fitting {Linear} {Mixed}-{Effects} {Models} {Using} lme4},
	volume = {67},
	copyright = {Copyright (c) 2015 Douglas Bates, Martin Mächler, Ben Bolker, Steve Walker},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v067i01},
	doi = {10.18637/jss.v067.i01},
	language = {en},
	number = {1},
	urldate = {2020-10-27},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	month = oct,
	year = {2015},
	note = {Number: 1},
	keywords = {Cholesky decomposition, linear mixed models, penalized least squares, sparse matrix methods},
	pages = {1--48},
}

@article{matuschek_balancing_2017,
	title = {Balancing {Type} {I} error and power in linear mixed models},
	volume = {94},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X17300013},
	doi = {10.1016/j.jml.2017.01.001},
	abstract = {Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments. Although LMMs have many advantages over ANOVA, like ANOVAs, setting them up for data analysis also requires some care. One simple option, when numerically possible, is to fit the full variance-covariance structure of random effects (the maximal model; Barr, Levy, Scheepers \& Tily, 2013), presumably to keep Type I error down to the nominal α in the presence of random effects. Although it is true that fitting a model with only random intercepts may lead to higher Type I error, fitting a maximal model also has a cost: it can lead to a significant loss of power. We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data, higher power is achieved without inflating Type I error rate if a model selection criterion is used to select a random effect structure that is supported by the data.},
	language = {en},
	urldate = {2020-10-27},
	journal = {Journal of Memory and Language},
	author = {Matuschek, Hannes and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald and Bates, Douglas},
	month = jun,
	year = {2017},
	keywords = {Hypothesis testing, Linear mixed effect model, Power},
	pages = {305--315},
}

@article{sassenhagen_common_2016,
	title = {A common misapplication of statistical inference: {Nuisance} control with null-hypothesis significance tests},
	volume = {162},
	issn = {0093-934X},
	shorttitle = {A common misapplication of statistical inference},
	url = {http://www.sciencedirect.com/science/article/pii/S0093934X16300323},
	doi = {10.1016/j.bandl.2016.08.001},
	abstract = {Experimental research on behavior and cognition frequently rests on stimulus or subject selection where not all characteristics can be fully controlled, even when attempting strict matching. For example, when contrasting patients to controls, variables such as intelligence or socioeconomic status are often correlated with patient status. Similarly, when presenting word stimuli, variables such as word frequency are often correlated with primary variables of interest. One procedure very commonly employed to control for such nuisance effects is conducting inferential tests on confounding stimulus or subject characteristics. For example, if word length is not significantly different for two stimulus sets, they are considered as matched for word length. Such a test has high error rates and is conceptually misguided. It reflects a common misunderstanding of statistical tests: interpreting significance not to refer to inference about a particular population parameter, but about 1. the sample in question, 2. the practical relevance of a sample difference (so that a nonsignificant test is taken to indicate evidence for the absence of relevant differences). We show inferential testing for assessing nuisance effects to be inappropriate both pragmatically and philosophically, present a survey showing its high prevalence, and briefly discuss an alternative in the form of regression including nuisance variables.},
	language = {en},
	urldate = {2020-10-27},
	journal = {Brain and Language},
	author = {Sassenhagen, Jona and Alday, Phillip M.},
	month = nov,
	year = {2016},
	pages = {42--45},
}

@article{jaeger_categorical_2008,
	series = {Special {Issue}: {Emerging} {Data} {Analysis}},
	title = {Categorical data analysis: {Away} from {ANOVAs} (transformation or not) and towards logit mixed models},
	volume = {59},
	issn = {0749-596X},
	shorttitle = {Categorical data analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X07001337},
	doi = {10.1016/j.jml.2007.11.007},
	abstract = {This paper identifies several serious problems with the widespread use of ANOVAs for the analysis of categorical outcome variables such as forced-choice variables, question-answer accuracy, choice in production (e.g. in syntactic priming research), et cetera. I show that even after applying the arcsine-square-root transformation to proportional data, ANOVA can yield spurious results. I discuss conceptual issues underlying these problems and alternatives provided by modern statistics. Specifically, I introduce ordinary logit models (i.e. logistic regression), which are well-suited to analyze categorical data and offer many advantages over ANOVA. Unfortunately, ordinary logit models do not include random effect modeling. To address this issue, I describe mixed logit models (Generalized Linear Mixed Models for binomially distributed outcomes, Breslow and Clayton [Breslow, N. E. \& Clayton, D. G. (1993). Approximate inference in generalized linear mixed models. Journal of the American Statistical Society 88(421), 9–25]), which combine the advantages of ordinary logit models with the ability to account for random subject and item effects in one step of analysis. Throughout the paper, I use a psycholinguistic data set to compare the different statistical methods.},
	language = {en},
	number = {4},
	urldate = {2020-10-27},
	journal = {Journal of Memory and Language},
	author = {Jaeger, T. Florian},
	month = nov,
	year = {2008},
	keywords = {Arcsine-square-root transformation, Categorical data analysis, Logistic regression, Mixed logit models},
	pages = {434--446},
}

@article{yarkoni_choosing_2017,
	title = {Choosing {Prediction} {Over} {Explanation} in {Psychology}: {Lessons} {From} {Machine} {Learning}},
	volume = {12},
	issn = {1745-6916},
	shorttitle = {Choosing {Prediction} {Over} {Explanation} in {Psychology}},
	url = {https://doi.org/10.1177/1745691617693393},
	doi = {10.1177/1745691617693393},
	abstract = {Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.},
	language = {en},
	number = {6},
	urldate = {2020-10-27},
	journal = {Perspectives on Psychological Science},
	author = {Yarkoni, Tal and Westfall, Jacob},
	month = nov,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	pages = {1100--1122},
}

@article{schad_how_2020,
	title = {How to capitalize on a priori contrasts in linear (mixed) models: {A} tutorial},
	volume = {110},
	issn = {0749-596X},
	shorttitle = {How to capitalize on a priori contrasts in linear (mixed) models},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X19300695},
	doi = {10.1016/j.jml.2019.104038},
	abstract = {Factorial experiments in research on memory, language, and in other areas are often analyzed using analysis of variance (ANOVA). However, for effects with more than one numerator degrees of freedom, e.g., for experimental factors with more than two levels, the ANOVA omnibus F-test is not informative about the source of a main effect or interaction. Because researchers typically have specific hypotheses about which condition means differ from each other, a priori contrasts (i.e., comparisons planned before the sample means are known) between specific conditions or combinations of conditions are the appropriate way to represent such hypotheses in the statistical model. Many researchers have pointed out that contrasts should be “tested instead of, rather than as a supplement to, the ordinary ‘omnibus’ F test” (Hays, 1973, p. 601). In this tutorial, we explain the mathematics underlying different kinds of contrasts (i.e., treatment, sum, repeated, polynomial, custom, nested, interaction contrasts), discuss their properties, and demonstrate how they are applied in the R System for Statistical Computing (R Core Team, 2018). In this context, we explain the generalized inverse which is needed to compute the coefficients for contrasts that test hypotheses that are not covered by the default set of contrasts. A detailed understanding of contrast coding is crucial for successful and correct specification in linear models (including linear mixed models). Contrasts defined a priori yield far more useful confirmatory tests of experimental hypotheses than standard omnibus F-tests. Reproducible code is available from https://osf.io/7ukf6/.},
	language = {en},
	urldate = {2020-10-27},
	journal = {Journal of Memory and Language},
	author = {Schad, Daniel J. and Vasishth, Shravan and Hohenstein, Sven and Kliegl, Reinhold},
	month = feb,
	year = {2020},
	keywords = {A priori hypotheses, Contrasts, Linear models, Null hypothesis significance testing},
	pages = {104038},
}

@article{baayen_mixed-effects_2008,
	series = {Special {Issue}: {Emerging} {Data} {Analysis}},
	title = {Mixed-effects modeling with crossed random effects for subjects and items},
	volume = {59},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X07001398},
	doi = {10.1016/j.jml.2007.12.005},
	abstract = {This paper provides an introduction to mixed-effects models for the analysis of repeated measurement data with subjects and items as crossed random effects. A worked-out example of how to use recent software for mixed-effects modeling is provided. Simulation studies illustrate the advantages offered by mixed-effects analyses compared to traditional analyses based on quasi-F tests, by-subjects analyses, combined by-subjects and by-items analyses, and random regression. Applications and possibilities across a range of domains of inquiry are discussed.},
	language = {en},
	number = {4},
	urldate = {2020-10-27},
	journal = {Journal of Memory and Language},
	author = {Baayen, R. H. and Davidson, D. J. and Bates, D. M.},
	month = nov,
	year = {2008},
	keywords = {By-item, By-subject, Crossed random effects, Mixed-effects models, Quasi-F},
	pages = {390--412},
}

@article{coleman_generalizing_1964,
	title = {Generalizing to a {Language} {Population}},
	volume = {14},
	issn = {0033-2941},
	url = {https://doi.org/10.2466/pr0.1964.14.1.219},
	doi = {10.2466/pr0.1964.14.1.219},
	abstract = {Many studies of verbal behavior have little scientific point if their conclusions have to be restricted to the specific language materials that were used in the experiment. It has not been customary, however, to perform significance tests that permit generalization beyond these specific materials, and thus there is little statistical evidence that such studies could be successfully replicated if a different sample of language materials were used. Three tests are described that will allow generalization to a population of language materials.},
	language = {en},
	number = {1},
	urldate = {2020-10-27},
	journal = {Psychological Reports},
	author = {Coleman, E. B.},
	month = feb,
	year = {1964},
	note = {Publisher: SAGE Publications Inc},
	pages = {219--226},
}

@article{clark_language-as-fixed-effect_1973,
	title = {The language-as-fixed-effect fallacy: {A} critique of language statistics in psychological research},
	volume = {12},
	issn = {0022-5371},
	shorttitle = {The language-as-fixed-effect fallacy},
	url = {http://www.sciencedirect.com/science/article/pii/S0022537173800143},
	doi = {10.1016/S0022-5371(73)80014-3},
	abstract = {Current investigators of words, sentences, and other language materials almost never provide statistical evidence that their findings generalize beyond the specific sample of language materials they have chosen. Nevertheless, these same investigators do not hesitate to conclude that their findings are true for language in general. In so doing, it is argued, they are committing the language-as-fixed-effect fallacy, which can lead to serious error. The problem is illustrated for one well-known series of studies in semantic memory. With the appropriate statistics these studies are shown to provide no reliable evidence for most of the main conclusions drawn from them. A review of other experiments in semantic memory shows that many of them are likewise suspect. It is demonstrated how this fallacy can be avoided by doing the right statistics, selecting the appropriate design, and sampling by systematic procedures, or, alternatively, by proceeding according to the so-called method of single cases.},
	language = {en},
	number = {4},
	urldate = {2020-10-27},
	journal = {Journal of Verbal Learning and Verbal Behavior},
	author = {Clark, Herbert H.},
	month = aug,
	year = {1973},
	pages = {335--359},
}

@article{judd_treating_2012,
	title = {Treating stimuli as a random factor in social psychology: {A} new and comprehensive solution to a pervasive but largely ignored problem},
	volume = {103},
	issn = {1939-1315(Electronic),0022-3514(Print)},
	shorttitle = {Treating stimuli as a random factor in social psychology},
	doi = {10.1037/a0028347},
	abstract = {Throughout social and cognitive psychology, participants are routinely asked to respond in some way to experimental stimuli that are thought to represent categories of theoretical interest. For instance, in measures of implicit attitudes, participants are primed with pictures of specific African American and White stimulus persons sampled in some way from possible stimuli that might have been used. Yet seldom is the sampling of stimuli taken into account in the analysis of the resulting data, in spite of numerous warnings about the perils of ignoring stimulus variation (Clark, 1973; Kenny, 1985; Wells \& Windschitl, 1999). Part of this failure to attend to stimulus variation is due to the demands imposed by traditional analysis of variance procedures for the analysis of data when both participants and stimuli are treated as random factors. In this article, we present a comprehensive solution using mixed models for the analysis of data with crossed random factors (e.g., participants and stimuli). We show the substantial biases inherent in analyses that ignore one or the other of the random factors, and we illustrate the substantial advantages of the mixed models approach with both hypothetical and actual, well-known data sets in social psychology (Bem, 2011; Blair, Chapleau, \& Judd, 2005; Correll, Park, Judd, \& Wittenbrink, 2002). (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Journal of Personality and Social Psychology},
	author = {Judd, Charles M. and Westfall, Jacob and Kenny, David A.},
	year = {2012},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Models, Random Sampling, Social Psychology, Stimulus Variability},
	pages = {54--69},
}

@article{cumming_new_2014,
	title = {The {New} {Statistics}: {Why} and {How}},
	volume = {25},
	issn = {0956-7976},
	shorttitle = {The {New} {Statistics}},
	url = {https://doi.org/10.1177/0956797613504966},
	doi = {10.1177/0956797613504966},
	abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
	language = {en},
	number = {1},
	urldate = {2020-10-27},
	journal = {Psychological Science},
	author = {Cumming, Geoff},
	month = jan,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	pages = {7--29},
}

@article{burki_accounting_2018,
	title = {Accounting for stimulus and participant effects in event-related potential analyses to increase the replicability of studies},
	volume = {309},
	issn = {01650270},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165027018302772},
	doi = {10.1016/j.jneumeth.2018.09.016},
	language = {en},
	urldate = {2020-10-27},
	journal = {Journal of Neuroscience Methods},
	author = {Bürki, Audrey and Frossard, Jaromil and Renaud, Olivier},
	month = nov,
	year = {2018},
	pages = {218--227},
}

@article{judd_experiments_2017,
	title = {Experiments with {More} {Than} {One} {Random} {Factor}: {Designs}, {Analytic} {Models}, and {Statistical} {Power}},
	volume = {68},
	issn = {0066-4308, 1545-2085},
	shorttitle = {Experiments with {More} {Than} {One} {Random} {Factor}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-122414-033702},
	doi = {10.1146/annurev-psych-122414-033702},
	language = {en},
	number = {1},
	urldate = {2020-11-29},
	journal = {Annual Review of Psychology},
	author = {Judd, Charles M. and Westfall, Jacob and Kenny, David A.},
	month = jan,
	year = {2017},
	pages = {601--625},
}

@article{henderson_analysis_1982,
	title = {Analysis of {Covariance} in the {Mixed} {Model}: {Higher}-{Level}, {Nonhomogeneous}, and {Random} {Regressions}},
	volume = {38},
	issn = {0006341X},
	shorttitle = {Analysis of {Covariance} in the {Mixed} {Model}},
	url = {https://www.jstor.org/stable/2530044?origin=crossref},
	doi = {10.2307/2530044},
	number = {3},
	urldate = {2020-11-30},
	journal = {Biometrics},
	author = {Henderson, Charles R.},
	month = sep,
	year = {1982},
	pages = {623},
}

@article{henderson_sire_1973,
	title = {Sire {Evaluation} and {Genetic} {Trends}},
	volume = {1973},
	issn = {0021-8812, 1525-3163},
	url = {https://academic.oup.com/jas/article/1973/Symposium/10-41/4697208},
	doi = {10.1093/ansci/1973.Symposium.10},
	language = {en},
	number = {Symposium},
	urldate = {2020-11-30},
	journal = {Journal of Animal Science},
	author = {Henderson, C. R.},
	month = jan,
	year = {1973},
	pages = {10--41},
}

@article{laird_random-effects_1982,
	title = {Random-{Effects} {Models} for {Longitudinal} {Data}},
	volume = {38},
	issn = {0006341X},
	url = {https://www.jstor.org/stable/2529876?origin=crossref},
	doi = {10.2307/2529876},
	number = {4},
	urldate = {2020-11-30},
	journal = {Biometrics},
	author = {Laird, Nan M. and Ware, James H.},
	month = dec,
	year = {1982},
	pages = {963},
}

@article{westfall_statistical_2014,
	title = {Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli.},
	volume = {143},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000014},
	doi = {10.1037/xge0000014},
	language = {en},
	number = {5},
	urldate = {2020-11-30},
	journal = {Journal of Experimental Psychology: General},
	author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
	year = {2014},
	pages = {2020--2045},
}

@article{brysbaert_power_2018,
	title = {Power {Analysis} and {Effect} {Size} in {Mixed} {Effects} {Models}: {A} {Tutorial}},
	volume = {1},
	issn = {2514-4820},
	shorttitle = {Power {Analysis} and {Effect} {Size} in {Mixed} {Effects} {Models}},
	url = {http://www.journalofcognition.org/articles/10.5334/joc.10/},
	doi = {10.5334/joc.10},
	language = {en},
	number = {1},
	urldate = {2020-11-30},
	journal = {Journal of Cognition},
	author = {Brysbaert, Marc and Stevens, Michaël},
	month = jan,
	year = {2018},
	pages = {9},
}

@article{brehm_contrast_2022,
	title = {Contrast coding choices in a decade of mixed models},
	volume = {125},
	issn = {0749596X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X22000213},
	doi = {10.1016/j.jml.2022.104334},
	language = {en},
	urldate = {2023-06-07},
	journal = {Journal of Memory and Language},
	author = {Brehm, Laurel and Alday, Phillip M.},
	month = aug,
	year = {2022},
	pages = {104334},
}


@article{cohen_earth_1994,
	title = {The earth is round (p {\textless} .05).},
	volume = {49},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0003-066X.49.12.997},
	doi = {10.1037/0003-066X.49.12.997},
	language = {en},
	number = {12},
	urldate = {2020-10-27},
	journal = {American Psychologist},
	author = {Cohen, Jacob},
	year = {1994},
	pages = {997--1003},
}

@incollection{Cohen_things_1992,
	address = {Washington},
	title = {Things {I} have learned (so far).},
	isbn = {978-1-55798-154-7 978-1-55798-167-7},
	url = {http://content.apa.org/books/10109-028},
	language = {en},
	urldate = {2020-10-27},
	booktitle = {Methodological issues \& strategies in clinical research.},
	publisher = {American Psychological Association},
	author = {Cohen, Jacob},
	editor = {Kazdin, Alan E.},
	year = {1992},
	doi = {10.1037/10109-028},
	pages = {315--333},
}
